# Transformer 기반 영-한 번역 모델 구현 보고서

## 1. 구현 설명 (Implementation Details)

### 1.1. 모델 아키텍처 (Model Architecture)

`nn.Transformer`와 같은 고수준 API를 사용하지 않고, 트랜스포머의 핵심 모듈을 클래스 단위로 직접 설계하여 모델의 작동 원리를 명확히 구현했습니다.

- **Positional Encoding**: 순서 정보가 없는 Self-Attention의 한계를 극복하기 위해 **Sin/Cos 주기 함수**를 이용해 위치 정보를 주입했습니다.
  - **수식**:
    $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
    $$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

- **Multi-Head Attention**: 입력 차원을 여러 개의 헤드로 나누어 병렬적으로 연산함으로써, 문장 내 다양한 문맥적 관계(Semantic & Syntactic)를 동시에 포착합니다.

- **Masking 시스템**:
  - **src_mask**: 패딩 토큰(`<pad>`)이 어텐션 연산에 포함되지 않도록 가려 불필요한 연산을 방지합니다.
  - **tgt_mask**: 디코더가 학습 시 현재 시점보다 미래의 단어를 미리 보지 못하도록 가리는 **Look-ahead 마스크**를 적용했습니다.

- **Residual Connection & LayerNorm**: 각 서브 레이어의 출력을 입력값과 더하고(잔차 연결) 정규화(Layer Normalization)함으로써, 층이 깊어져도 기울기 소실 문제 없이 안정적으로 학습되도록 설계했습니다.

---

### 1.2. 데이터 파이프라인 (Data Pipeline)

- **토큰화(Tokenization)**: 단순 **공백(`split()`) 기반 토큰화**를 사용하여 모델의 복잡도를 낮추고, 소규모 데이터셋에서의 학습 가능성을 확인했습니다.
- **단어장(Vocabulary)**: 학습 데이터 내 빈도를 바탕으로 `src_vocab_map`과 `tgt_vocab_map`을 생성했습니다. 특수 토큰으로 `<pad>`, `<sos>`, `<eos>`, `<unk>`를 포함하여 문장의 시작과 끝, 모르는 단어를 처리하도록 했습니다.
- **전처리**: 모든 문장은 **MAX_LEN(40)**에 맞춰 길이를 통일(Padding)했으며, 디코더 학습 효율을 높이기 위해 정답 문장을 다음 단어 예측의 입력으로 넣는 **Teacher Forcing** 기법을 적용했습니다.

---

### 1.3. 학습 전략 (Training Strategy)

성능과 학습 속도의 균형을 맞추기 위해 다음과 같은 환경에서 학습을 진행했습니다.

| 항목              | 설정 내용                                  |
| :---------------- | :----------------------------------------- |
| **Optimizer**     | Adam (Learning Rate: 0.0005)               |
| **Loss Function** | CrossEntropyLoss (패딩 토큰 인덱스 0 제외) |
| **Device**        | CUDA (GPU 가속 활용)                       |

#### 하이퍼파라미터 (Hyperparameters)

- **d_model**: 256
- **n_head**: 8
- **num_layers**: 2
- **Epochs**: 50
- **Batch_size**: 128

---

## 2. 실험 결과 요약 (Experimental Results)

### 2.1. 학습 지표 (Training Metrics)

50회 반복 학습(Epoch) 결과, 손실값(Loss)은 다음과 같은 추이를 보이며 안정적으로 수렴하였습니다.

| Epoch | Loss   | 비고                              |
| :---- | :----- | :-------------------------------- |
| 10    | 3.3463 | 초기 학습 단계                    |
| 30    | 0.0737 | 손실값의 급격한 하락 및 수렴 시작 |
| 50    | 0.0204 | 최종 수렴 단계                    |

> **주석**: 학습 손실값은 안정적으로 하락하였으나, 데이터셋의 규모(약 4,500개)가 작아 검증 데이터셋에서의 일반화 성능은 다소 제한적일 것으로 판단됩니다. 향후 대규모 코퍼스를 통한 추가 학습이 필요합니다.

---

### 2.2. 번역 성능 분석

학습된 모델의 추론(Inference) 능력을 확인하기 위해 임의의 문장을 입력하여 테스트를 진행했습니다. 학습 초기 단계에서는 무작위 단어가 나열되었으나, 학습이 진행됨에 따라 점차 문법적 구조를 갖추려는 경향을 보였습니다.

#### 테스트 결과 예시

##### 학습 없이 입력

- **입력(English)**: `Yes, how is it?`
- **출력(Korean)**: `효과도 마련할 핸드폰이 규격으로 드라마 파란 걸렸잖아. 태극기를 집들이는 약입니다. 간장에다가 경유해야 발생했더 라고요. 자식한테 셋, 편의 재무팀에 장단점도 자식한테 장비를 것이라고 점을 이왕이면 빠르더라. 마음처럼 대로 안전과 프로그램이었어요. 드려요. 유학생 금지됐잖아. 낀 버스를 예의범절을 동료들이랑 어려워질 어려워질 어려워질 어려워질 어려워질`

##### 학습 후 입력

- **입력(English)**: `Yes, how is it?`
- **출력(Korean)**: `네, 어때요?`

##### 학습 후 변형

- **입력(English)**: `Yes, how is it today?`
- **출력(Korean)**: `네, 어떻게 말할지`

**분석**:
단순 공백 기반 토큰화와 적은 데이터량으로 인해 완벽한 번역에는 한계가 있습니다. 학습에 쓰인 문장을 조금만 바꿔도 제대로 된 번역이 나오지 않는 문제가 발생합니다. 그래도 학습을 한 후 문장을 넣으면 대응되는 단어를 찾으려고 합니다.
